%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Avital Sternin at 2016-03-21 13:40:10 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{schultz_learning_2004,
	Author = {Schultz, M. and Joachims, T.},
	Date-Added = {2016-03-21 17:39:47 +0000},
	Date-Modified = {2016-03-21 17:39:47 +0000},
	File = {[PDF] from wustl.edu:/Users/sstober/work/zotero/storage/ANTVC69K/Schultz and Joachims - 2004 - Learning a distance metric from relative compariso.pdf:application/pdf;Snapshot:/Users/sstober/work/zotero/storage/NC537BX2/books.html:text/html},
	Journal = {Advances in neural information processing systems (NIPS)},
	Pages = {41--48},
	Title = {Learning a distance metric from relative comparisons},
	Urldate = {2015-06-03},
	Xauthor = {Schultz, Matthew and Joachims, Thorsten},
	Xurl = {https://books.google.com/books?hl=en&lr=&id=0F-9C7K8fQ8C&oi=fnd&pg=PA41&dq=Learning+a+Distance+Metric+from+Relative+Comparisons&ots=THGtnXXc46&sig=RUHKl3-9LGQ9PylH7h6D9ZzB18E},
	Year = {2004}}

@article{hinton_improving_2012,
	Abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	Author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	Date-Added = {2015-09-25 16:26:18 +0000},
	Date-Modified = {2015-09-25 16:26:18 +0000},
	File = {arXiv\:1207.0580 PDF:/Users/sstober/work/zotero/storage/CXJUW2FN/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf:application/pdf;arXiv.org Snapshot:/Users/sstober/work/zotero/storage/MTE9KQU5/1207.html:text/html},
	Journal = {arXiv:1207.0580 [cs]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Note = {arXiv: 1207.0580},
	Title = {Improving neural networks by preventing co-adaptation of feature detectors},
	Url = {http://arxiv.org/abs/1207.0580},
	Urldate = {2015-04-26},
	Xmonth = jul,
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1207.0580}}

@article{tang_deep_2013,
	Abstract = {Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these "deep learning" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.},
	Annote = {Comment: Contribution to the ICML 2013 Challenges in Representation Learning Workshop},
	Author = {Tang, Yichuan},
	Date-Added = {2015-09-25 16:17:14 +0000},
	Date-Modified = {2015-09-25 16:17:14 +0000},
	File = {arXiv\:1306.0239 PDF:/Users/sstober/work/zotero/storage/G8I7FHCV/Tang - 2013 - Deep Learning using Linear Support Vector Machines.pdf:application/pdf;arXiv.org Snapshot:/Users/sstober/work/zotero/storage/RN7JSDGE/1306.html:text/html},
	Journal = {arXiv:1306.0239 [cs, stat]},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Note = {arXiv: 1306.0239},
	Title = {Deep {Learning} using {Linear} {Support} {Vector} {Machines}},
	Url = {http://arxiv.org/abs/1306.0239},
	Urldate = {2015-05-05},
	Xmonth = jun,
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1306.0239}}

@article{goodfellow_pylearn2_2013,
	Abstract = {Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.},
	Annote = {Comment: 9 pages},
	Author = {Goodfellow, Ian J. and Warde-Farley, David and Lamblin, Pascal and Dumoulin, Vincent and Mirza, Mehdi and Pascanu, Razvan and Bergstra, James and Bastien, Fr{\'e}d{\'e}ric and Bengio, Yoshua},
	Date-Added = {2015-09-25 15:58:45 +0000},
	Date-Modified = {2015-09-25 15:59:38 +0000},
	File = {arXiv\:1308.4214 PDF:/Users/sstober/work/zotero/storage/J8AHM2IN/Goodfellow et al. - 2013 - Pylearn2 a machine learning research library.pdf:application/pdf;arXiv.org Snapshot:/Users/sstober/work/zotero/storage/UN2MXDFE/1308.html:text/html},
	Journal = {arXiv:1308.4214 [cs, stat]},
	Keywords = {Computer Science - Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
	Note = {arXiv: 1308.4214},
	Shorttitle = {Pylearn2},
	Title = {Pylearn2: a machine learning research library},
	Url = {http://arxiv.org/abs/1308.4214},
	Urldate = {2015-04-26},
	Xmonth = aug,
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1308.4214}}

@article{ellis_beat_2007,
	Abstract = {Beat tracking i.e. deriving from a music audio signal a sequence of beat instants that might correspond to when a human listener would tap his foot involves satisfying two con- straints: On the one hand, the selected instants should generally correspond to moments in the audio where a beat is indicated, for instance by the onset of a note played by one of the instru- ments. On the other hand, the set of beats should reflect a locally-constant inter-beat-interval, since it is this regular spacing between beat times that defines musical rhythm. These dual constraints map neatly onto the two constraints optimized in dynamic programming, the local match, and the transition cost. We describe a beat tracking system which first estimates a global tempo, uses this tempo to construct a transition cost function, then uses dynamic programming to find the best-scoring set of beat times that reflect the tempo as well as corresponding to moments of high onset strength in a function derived from the audio. This very simple and computationally efficient procedure is shown to perform well on the MIREX-06 beat track- ing training data, achieving an average beat accuracy of just under 60\% on the development data. We also examine the impact of the assumption of a fixed target tempo, and show that the system is typically able to track tempo changes in a range of 10\% of the target tempo.},
	Author = {Ellis, Daniel P. W.},
	Date-Added = {2015-09-25 00:25:14 +0000},
	Date-Modified = {2015-09-25 00:25:14 +0000},
	Doi = {10.1080/09298210701653344},
	Journal = {Journal of New Music Research},
	Number = {1},
	Pages = {51--60},
	Title = {Beat {Tracking} by {Dynamic} {Programming}},
	Url = {http://www.tandfonline.com/doi/pdf/10.1080/09298210701653344},
	Volume = {36},
	Xmonth = may,
	Year = {2007},
	Bdsk-Url-1 = {http://www.tandfonline.com/doi/pdf/10.1080/09298210701653344},
	Bdsk-Url-2 = {http://dx.doi.org/10.1080/09298210701653344}}

@article{lee_independent_1999,
	Abstract = {An extension of the infomax algorithm of Bell and Sejnowski (1995) is presented that is able blindly to separate mixed signals with sub- and supergaussian source distributions. This was achieved by using a simple type of learning rule first derived by Girolami (1997) by choosing negentropy as a projection pursuit index. Parameterized probability distributions that have sub- and supergaussian regimes were used to derive a general learning rule that preserves the simple architecture proposed by Bell and Sejnowski (1995), is optimized using the natural gradient by Amari (1998), and uses the stability analysis of Cardoso and Laheld (1996) to switch between sub- and supergaussian regimes. We demonstrate that the extended infomax algorithm is able to separate 20 sources with a variety of source distributions easily. Applied to high-dimensional data from electroencephalographic recordings, it is effective at separating artifacts such as eye blinks and line noise from weaker electrical signals that arise from sources in the brain.},
	Author = {Lee, Te-Won and Girolami, Mark and Sejnowski, Terrence J.},
	Date-Added = {2015-09-25 00:25:06 +0000},
	Date-Modified = {2015-09-25 00:25:06 +0000},
	Doi = {10.1162/089976699300016719},
	File = {Neural Computation Snapshot:/Users/sstober/work/zotero/storage/NSQJHXV5/089976699300016719.html:text/html},
	Issn = {0899-7667},
	Journal = {Neural Computation},
	Number = {2},
	Pages = {417--441},
	Title = {Independent {Component} {Analysis} {Using} an {Extended} {Infomax} {Algorithm} for {Mixed} {Subgaussian} and {Supergaussian} {Sources}},
	Url = {http://dx.doi.org/10.1162/089976699300016719},
	Urldate = {2015-04-27},
	Volume = {11},
	Xmonth = feb,
	Year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/089976699300016719}}

@article{perrin_spherical_1989,
	Abstract = {Description of mapping methods using spherical splines, both to interpolate scalp potentials (SPs), and to approximate scalp current densities (SCDs). Compared to a previously published method using thin plate splines, the advantages are a very simple derivation of the SCD approximation, faster computing times, and greater accuracy in areas with few electrodes.},
	Author = {Perrin, F. and Pernier, J. and Bertrand, O. and Echallier, J. F.},
	Date-Added = {2015-09-25 00:25:03 +0000},
	Date-Modified = {2015-09-25 00:25:03 +0000},
	Doi = {10.1016/0013-4694(89)90180-6},
	File = {ScienceDirect Full Text PDF:/Users/sstober/work/zotero/storage/S9HFHSMG/Perrin et al. - 1989 - Spherical splines for scalp potential and current .pdf:application/pdf;ScienceDirect Snapshot:/Users/sstober/work/zotero/storage/D733M8IG/0013469489901806.html:text/html},
	Issn = {0013-4694},
	Journal = {Electroencephalography and Clinical Neurophysiology},
	Keywords = {EEG or evoked potential mapping, Scalp current density, Spherical splines},
	Number = {2},
	Pages = {184--187},
	Title = {Spherical splines for scalp potential and current density mapping},
	Url = {http://www.sciencedirect.com/science/article/pii/0013469489901806},
	Urldate = {2015-07-16},
	Volume = {72},
	Xmonth = feb,
	Year = {1989},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0013469489901806},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/0013-4694(89)90180-6}}

@article{teplan_fundamentals_2002,
	Author = {Teplan, Michal},
	Date-Added = {2015-09-25 00:24:59 +0000},
	Date-Modified = {2015-09-25 00:24:59 +0000},
	File = {[PDF] from edumed.org.br:/Users/sstober/work/zotero/storage/I9B6M6TP/Teplan - 2002 - Fundamentals of EEG measurement.pdf:application/pdf},
	Journal = {Measurement science review},
	Number = {2},
	Pages = {1--11},
	Title = {Fundamentals of {EEG} measurement},
	Url = {http://www.edumed.org.br/cursos/neurociencia/MethodsEEGMeasurement.pdf},
	Urldate = {2015-04-27},
	Volume = {2},
	Year = {2002},
	Bdsk-Url-1 = {http://www.edumed.org.br/cursos/neurociencia/MethodsEEGMeasurement.pdf}}

@article{willander_imagery_scale_2010,
	Author = {Willander, J and Baraldi, S},
	Date-Added = {2015-09-25 00:24:53 +0000},
	Date-Modified = {2015-09-25 00:24:53 +0000},
	Journal = {Behaviour Research Methods},
	Number = {3},
	Pages = {785-590},
	Title = {Development of a new Clarity of Auditory Imagery Scale},
	Volume = {42},
	Year = {2010}}

@article{mullensiefen_musicality_2014,
	Abstract = {Musical skills and expertise vary greatly in Western societies. Individuals can differ in their repertoire of musical behaviours as well as in the level of skill they display for any single musical behaviour. The types of musical behaviours we refer to here are broad, ranging from performance on an instrument and listening expertise, to the ability to employ music in functional settings or to communicate about music. In this paper, we first describe the concept of `musical sophistication' which can be used to describe the multi-faceted nature of musical expertise. Next, we develop a novel measurement instrument, the Goldsmiths Musical Sophistication Index (Gold-MSI) to assess self-reported musical skills and behaviours on multiple dimensions in the general population using a large Internet sample (n = 147,636). Thirdly, we report results from several lab studies, demonstrating that the Gold-MSI possesses good psychometric properties, and that self-reported musical sophistication is associated with performance on two listening tasks. Finally, we identify occupation, occupational status, age, gender, and wealth as the main socio-demographic factors associated with musical sophistication. Results are discussed in terms of theoretical accounts of implicit and statistical music learning and with regard to social conditions of sophisticated musical engagement.},
	Author = {M{\"u}llensiefen, Daniel and Gingras, Bruno and Musil, Jason and Stewart, Lauren},
	Date-Added = {2015-09-25 00:24:49 +0000},
	Date-Modified = {2015-09-25 00:24:49 +0000},
	Doi = {10.1371/journal.pone.0089642},
	File = {PubMed Central Full Text PDF:/Users/sstober/work/zotero/storage/3WZXAB88/M{\"u}llensiefen et al. - 2014 - The Musicality of Non-Musicians An Index for Asse.pdf:application/pdf},
	Issn = {1932-6203},
	Journal = {PLoS ONE},
	Number = {2},
	Pmcid = {PMC3935919},
	Pmid = {24586929},
	Shorttitle = {The {Musicality} of {Non}-{Musicians}},
	Title = {The {Musicality} of {Non}-{Musicians}: {An} {Index} for {Assessing} {Musical} {Sophistication} in the {General} {Population}},
	Url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3935919/},
	Urldate = {2015-04-27},
	Volume = {9},
	Xmonth = feb,
	Year = {2014},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3935919/},
	Bdsk-Url-2 = {http://dx.doi.org/10.1371/journal.pone.0089642}}

@incollection{masci_stacked_2011,
	Author = {Masci, Jonathan and Meier, Ueli and Cire{\c s}an, Dan and Schmidhuber, J{\"u}rgen},
	Booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning}--{ICANN} 2011},
	Date-Added = {2015-09-25 00:19:51 +0000},
	Date-Modified = {2015-09-25 00:19:51 +0000},
	File = {[PDF] from idsia.ch:/Users/sstober/work/zotero/storage/EQFUR284/Masci et al. - 2011 - Stacked convolutional auto-encoders for hierarchic.pdf:application/pdf;Snapshot:/Users/sstober/work/zotero/storage/68SGNMN5/978-3-642-21735-7_7.html:text/html},
	Pages = {52--59},
	Publisher = {Springer},
	Title = {Stacked convolutional auto-encoders for hierarchical feature extraction},
	Url = {http://link.springer.com/chapter/10.1007/978-3-642-21735-7_7},
	Urldate = {2015-05-05},
	Year = {2011},
	Bdsk-Url-1 = {http://link.springer.com/chapter/10.1007/978-3-642-21735-7_7}}

@article{hubel_receptive_1963,
	Author = {Hubel, D H and Wiesel, T N},
	Date-Added = {2015-09-25 00:19:27 +0000},
	Date-Modified = {2015-09-25 00:19:27 +0000},
	File = {:Users/avitalsternin/Documents/Mendeley Articles/Hubel, Wiesel - 1963 - Receptive fields of cells in striate cortex of very young, visually inexperienced kittens.pdf:pdf},
	Journal = {J. neurophysiol},
	Pages = {994--1002},
	Title = {{Receptive fields of cells in striate cortex of very young, visually inexperienced kittens}},
	Url = {http://www.inf.ed.ac.uk/teaching/courses/inf1-cg/readings/HubelWiesel1963Jneurophysiol.pdf},
	Volume = {26},
	Year = {1963},
	Bdsk-Url-1 = {http://www.inf.ed.ac.uk/teaching/courses/inf1-cg/readings/HubelWiesel1963Jneurophysiol.pdf}}

@article{schaefer_shared_2013,
	Abstract = {The current work investigates the brain activation shared between perception and imagery of music as measured with electroencephalography (EEG). Meta-analyses of four separate EEG experiments are presented, each focusing on perception and imagination of musical sound, with differing levels of stimulus complexity. Imagination and perception of simple accented metronome trains, as manifested in the clock illusion, as well as monophonic melodies are discussed, as well as more complex rhythmic patterns and ecologically natural music stimuli. By decomposing the data with principal component analysis (PCA), similar component distributions are found to explain most of the variance in each experiment. All data sets show a fronto-central and a more central component as the largest sources of variance, fitting with projections seen for the network of areas contributing to the N1/P2 complex. We expanded on these results using tensor decomposition. This allows us to add in the tasks to find shared activation, but does not make assumptions of independence or orthogonality and calculates the relative strengths of these components for each task. The components found in the PCA were shown to be further decomposable into parts that load primarily on to the perception or imagery task, or both, thereby adding more detail. It is shown that the frontal and central components have multiple parts that are differentially active during perception and imagination. A number of possible interpretations of these results are discussed, taking into account the different stimulus materials and measurement conditions.},
	Author = {Schaefer, Rebecca S. and Desain, Peter and Farquhar, Jason},
	Date-Added = {2015-09-25 00:18:29 +0000},
	Date-Modified = {2015-09-25 15:51:24 +0000},
	Doi = {10.1016/j.neuroimage.2012.12.064},
	File = {Schaefer et al. - 2013 - Shared processing of perception and imagery of mus.pdf:/Users/sstober/work/zotero/storage/2N4GXWVD/Schaefer et al. - 2013 - Shared processing of perception and imagery of mus.pdf:application/pdf;Schaefer et al. - 2013 - Supplements for Shared processing of perception and imagery of mus.pdf:/Users/sstober/work/zotero/storage/RKNUV593/Schaefer et al. - 2013 - Supplements for Shared processing of perception and imagery of mus.pdf:application/pdf;ScienceDirect Snapshot:/Users/sstober/work/zotero/storage/K67W7GCN/S1053811913000037.html:text/html},
	Issn = {1053-8119},
	Journal = {NeuroImage},
	Keywords = {Electroencephalography, Music imagination, Music perception, Parallel factor analysis, Principal component analysis},
	Pages = {317--326},
	Title = {Shared processing of perception and imagery of music in decomposed {EEG}},
	Url = {http://www.sciencedirect.com/science/article/pii/S1053811913000037},
	Urldate = {2015-04-25},
	Volume = {70},
	Xmonth = apr,
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1053811913000037},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.neuroimage.2012.12.064}}

@incollection{schaefer_single_2009,
	Author = {Schaefer, Rebecca S. and Blokland, Yvonne and Farquhar, Jason and Desain, Peter},
	Booktitle = {Proceedings of the 2009 {Berlin} {BCI} {Workshop}},
	Date-Added = {2015-09-25 00:18:29 +0000},
	Date-Modified = {2015-09-25 15:51:19 +0000},
	File = {Full Text PDF:/Users/sstober/work/zotero/storage/6G76QHN9/Desain - 2009 - Single trial classification of perceived and imagi.pdf:application/pdf},
	Title = {Single trial classification of perceived and imagined music from {EEG}},
	Year = {2009}}

@phdthesis{schaefer_measuring_2011,
	Author = {Schaefer, Rebecca S.},
	Date-Added = {2015-09-25 00:18:07 +0000},
	Date-Modified = {2015-09-25 15:51:34 +0000},
	File = {Schaefer - 2011 - Measuring the mind's ear EEG of music imagery.pdf:/Users/sstober/work/zotero/storage/9EX2AJI9/Schaefer - 2011 - Measuring the mind's ear EEG of music imagery.pdf:application/pdf},
	School = {Radboud University Nijmegen},
	Title = {Measuring the mind's ear {EEG} of music imagery},
	Xaddress = {Nijmegen, The Netherlands},
	Xisbn = {9789491027116 9491027115},
	Xlanguage = {English},
	Xpublisher = {s.n.] ; UB Nijmegen [host]},
	Year = {2011}}

@article{schaefer_name_2011,
	Abstract = {In the current study we use electroencephalography (EEG) to detect heard music from the brain signal, hypothesizing that the time structure in music makes it especially suitable for decoding perception from EEG signals. While excluding music with vocals, we classified the perception of seven different musical fragments of about three seconds, both individually and cross-participants, using only time domain information (the event-related potential, ERP). The best individual results are 70\% correct in a seven-class problem while using single trials, and when using multiple trials we achieve 100\% correct after six presentations of the stimulus. When classifying across participants, a maximum rate of 53\% was reached, supporting a general representation of each musical fragment over participants. While for some music stimuli the amplitude envelope correlated well with the ERP, this was not true for all stimuli. Aspects of the stimulus that may contribute to the differences between the EEG responses to the pieces of music are discussed.},
	Author = {Schaefer, Rebecca S. and Farquhar, Jason and Blokland, Yvonne and Sadakata, Makiko and Desain, Peter},
	Date-Added = {2015-09-25 00:18:07 +0000},
	Date-Modified = {2015-09-25 15:51:30 +0000},
	Doi = {10.1016/j.neuroimage.2010.05.084},
	File = {Schaefer et al. - 2011 - Name that tune Decoding music from the listening .pdf:/Users/sstober/work/zotero/storage/Q2XKCI2K/Schaefer et al. - 2011 - Name that tune Decoding music from the listening .pdf:application/pdf;ScienceDirect Snapshot:/Users/sstober/work/zotero/storage/WAD95PMI/S1053811910008402.html:text/html},
	Issn = {1053-8119},
	Journal = {NeuroImage},
	Keywords = {Electroencephalography, Music perception, Single trial classification},
	Number = {2},
	Pages = {843--849},
	Series = {Multivariate {Decoding} and {Brain} {Reading}},
	Shorttitle = {Name that tune},
	Title = {Name that tune: {Decoding} music from the listening brain},
	Url = {http://www.sciencedirect.com/science/article/pii/S1053811910008402},
	Urldate = {2015-04-25},
	Volume = {56},
	Xmonth = may,
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1053811910008402},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.neuroimage.2010.05.084}}

@article{herholz_2012,
	Abstract = {We used fMRI to investigate the neuronal correlates of encoding and recognizing heard and imagined melodies. Ten participants were shown lyrics of familiar verbal tunes; they either heard the tune along with the lyrics, or they had to imagine it. In a subsequent surprise recognition test, they had to identify the titles of tunes that they had heard or imagined earlier. The functional data showed substantial overlap during melody perception and imagery, including secondary auditory areas. During imagery compared with perception, an extended network including pFC, SMA, intraparietal sulcus, and cerebellum showed increased activity, in line with the increased processing demands of imagery. Functional connectivity of anterior right temporal cortex with frontal areas was increased during imagery compared with perception, indicating that these areas form an imagery-related network. Activity in right superior temporal gyrus and pFC was correlated with the subjective rating of imagery vividness. Similar to the encoding phase, the recognition task recruited overlapping areas, including inferior frontal cortex associated with memory retrieval, as well as left middle temporal gyrus. The results present new evidence for the cortical network underlying goal-directed auditory imagery, with a prominent role of the right pFC both for the subjective impression of imagery vividness and for on-line mental monitoring of imagery-related activity in auditory areas.},
	Author = {Herholz, SC and Halpern, AR and Zatorre, RJ},
	Date-Added = {2015-09-25 00:17:20 +0000},
	Date-Modified = {2015-09-25 00:17:20 +0000},
	Doi = {10.1162/jocn\_a\_00216},
	File = {:Users/avitalsternin/Documents/Mendeley Articles/Herholz, Halpern, Zatorre - 2012 - Neuronal correlates of perception, imagery, and memory for familiar tunes.pdf:pdf},
	Issn = {1530-8898},
	Journal = {Journal of cognitive neuroscience},
	Keywords = {Acoustic Stimulation,Acoustic Stimulation: methods,Adult,Auditory Cortex,Auditory Cortex: physiology,Auditory Perception,Auditory Perception: physiology,Female,Humans,Imagination,Imagination: physiology,Magnetic Resonance Imaging,Magnetic Resonance Imaging: methods,Male,Memory,Memory: physiology,Music,Music: psychology,Nerve Net,Nerve Net: physiology,Recognition (Psychology),Recognition (Psychology): physiology,Young Adult},
	Number = {6},
	Pages = {1382--97},
	Pmid = {22360595},
	Title = {{Neuronal correlates of perception, imagery, and memory for familiar tunes}},
	Url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00216 http://www.ncbi.nlm.nih.gov/pubmed/22360595},
	Volume = {24},
	Xmonth = jun,
	Year = {2012},
	Bdsk-Url-1 = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn%5C_a%5C_00216%20http://www.ncbi.nlm.nih.gov/pubmed/22360595},
	Bdsk-Url-2 = {http://dx.doi.org/10.1162/jocn%5C_a%5C_00216}}

@article{Herholz2008,
	Abstract = {Although the influence of long-term musical training on the processing of heard music has been the subject of many studies, the neural basis of music imagery and the effect of musical expertise remain insufficiently understood. By means of magnetoencephalography (MEG) we compared musicians and nonmusicians in a musical imagery task with familiar melodies. Subjects listened to the beginnings of the melodies, continued them in their imagination and then heard a tone which was either a correct or an incorrect further continuation of the melody. Only in musicians was the imagery of these melodies strong enough to elicit an early preattentive brain response to unexpected incorrect continuations of the imagined melodies; this response, the imagery mismatch negativity (iMMN), peaked approximately 175 ms after tone onset and was right-lateralized. In contrast to previous studies the iMMN was not based on a heard but on a purely imagined memory trace. Our results suggest that in trained musicians imagery and perception rely on similar neuronal correlates, and that the musicians' intense musical training has modified this network to achieve a superior ability for imagery and preattentive processing of music.},
	Author = {Herholz, SC and Lappe, Claudia and Knief, Arne and Pantev, Christo},
	Date-Added = {2015-09-25 00:17:11 +0000},
	Date-Modified = {2015-09-25 00:17:11 +0000},
	Doi = {10.1111/j.1460-9568.2008.06515.x},
	File = {:Users/avitalsternin/Documents/Mendeley Articles/Herholz et al. - 2008 - Neural basis of music imagery and the effect of musical expertise.pdf:pdf},
	Issn = {1460-9568},
	Journal = {The European journal of neuroscience},
	Keywords = {Acoustic Stimulation,Adult,Auditory,Auditory Perception,Auditory Perception: physiology,Auditory: physiology,Brain Mapping,Cerebral Cortex,Cerebral Cortex: physiology,Cues,Evoked Potentials,Evoked Potentials: physiology,Female,Functional Laterality,Functional Laterality: physiology,Humans,Imagination,Imagination: physiology,Learning,Learning: physiology,Magnetoencephalography,Male,Motor Skills,Motor Skills: physiology,Music,Music: psychology,Nerve Net,Nerve Net: physiology,Neuropsychological Tests,Pitch Perception,Pitch Perception: physiology,Reaction Time,Reaction Time: physiology,Young Adult},
	Month = dec,
	Number = {11},
	Pages = {2352--60},
	Pmid = {19046375},
	Title = {{Neural basis of music imagery and the effect of musical expertise.}},
	Url = {http://www.ncbi.nlm.nih.gov/pubmed/19046375},
	Volume = {28},
	Year = {2008},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pubmed/19046375},
	Bdsk-Url-2 = {http://dx.doi.org/10.1111/j.1460-9568.2008.06515.x}}

@article{Kraemer2005,
	Abstract = {Auditory imagery occurs when one mentally rehearses telephone numbers or has a song 'on the brain'--it is the subjective experience of hearing in the absence of auditory stimulation, and is useful for investigating aspects of human cognition. Here we use functional magnetic resonance imaging to identify and characterize the neural substrates that support unprompted auditory imagery and find that auditory and visual imagery seem to obey similar basic neural principles.},
	Author = {Kraemer, David J M and Macrae, C Neil and Green, Adam E and Kelley, William M},
	Date-Added = {2015-09-25 00:16:35 +0000},
	Date-Modified = {2015-09-25 00:16:35 +0000},
	Doi = {10.1038/434158a},
	File = {:Users/avitalsternin/Documents/Mendeley Articles/Kraemer et al. - 2005 - Musical imagery sound of silence activates auditory cortex.pdf:pdf},
	Issn = {1476-4687},
	Journal = {Nature},
	Keywords = {Acoustic Stimulation,Auditory Cortex,Auditory Cortex: physiology,Humans,Imagination,Imagination: physiology,Linguistics,Magnetic Resonance Imaging,Memory,Memory: physiology,Music,Visual Cortex,Visual Cortex: physiology},
	Month = mar,
	Number = {7030},
	Pages = {158},
	Pmid = {15758989},
	Title = {{Musical imagery: sound of silence activates auditory cortex.}},
	Url = {http://www.ncbi.nlm.nih.gov/pubmed/15758989},
	Volume = {434},
	Year = {2005},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pubmed/15758989},
	Bdsk-Url-2 = {http://dx.doi.org/10.1038/434158a}}

@article{halpern_fmri_2004,
	Abstract = {The generality of findings implicating secondary auditory areas in auditory imagery was tested by using a timbre imagery task with fMRI. Another aim was to test whether activity in supplementary motor area (SMA) seen in prior studies might have been related to subvocalization. Participants with moderate musical background were scanned while making similarity judgments about the timbre of heard or imagined musical instrument sounds. The critical control condition was a visual imagery task. The pattern of judgments in perceived and imagined conditions was similar, suggesting that perception and imagery access similar cognitive representations of timbre. As expected, judgments of heard timbres, relative to the visual imagery control, activated primary and secondary auditory areas with some right-sided asymmetry. Timbre imagery also activated secondary auditory areas relative to the visual imagery control, although less strongly, in accord with previous data. Significant overlap was observed in these regions between perceptual and imagery conditions. Because the visual control task resulted in deactivation of auditory areas relative to a silent baseline, we interpret the timbre imagery effect as a reversal of that deactivation. Despite the lack of an obvious subvocalization component to timbre imagery, some activity in SMA was observed, suggesting that SMA may have a more general role in imagery beyond any motor component.},
	Author = {Halpern, Andrea R and Zatorre, Robert J and Bouffard, Marc and Johnson, Jennifer A},
	Date-Added = {2015-09-25 00:15:26 +0000},
	Date-Modified = {2015-09-25 00:15:26 +0000},
	Doi = {10.1016/j.neuropsychologia.2003.12.017},
	File = {:Users/avitalsternin/Documents/Mendeley Articles/Halpern et al. - 2004 - Behavioral and neural correlates of perceived and imagined musical timbre.pdf:pdf},
	Isbn = {1570577129},
	Issn = {0028-3932},
	Journal = {Neuropsychologia},
	Keywords = {Acoustic Stimulation,Adult,Association,Auditory Cortex,Auditory Cortex: physiology,Auditory Perception,Auditory Perception: physiology,Brain Mapping,Female,Frontal Lobe,Frontal Lobe: physiology,Humans,Imagination,Imagination: physiology,Magnetic Resonance Imaging,Male,Motor Cortex,Motor Cortex: physiology,Music,Music: psychology,Photic Stimulation,Reference Values,Sound,Speech,Speech: physiology},
	Number = {9},
	Pages = {1281--92},
	Pmid = {15178179},
	Title = {{Behavioral and neural correlates of perceived and imagined musical timbre.}},
	Url = {http://www.ncbi.nlm.nih.gov/pubmed/15178179},
	Volume = {42},
	Xmonth = jan,
	Year = {2004},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pubmed/15178179},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.neuropsychologia.2003.12.017}}

@article{vlek_shared_2011,
	Abstract = {Objective
An auditory rhythm can be perceived as a sequence of accented (loud) and non-accented (soft) beats or it can be imagined. Subjective rhythmization refers to the induction of accenting patterns during the presentation of identical auditory pulses at an isochronous rate. It can be an automatic process, but it can also be voluntarily controlled. We investigated whether imagined accents can be decoded from brain signals on a single-trial basis, and if there is information shared between perception and imagery in the contrast of accents and non-accents.
Methods
Ten subjects perceived and imagined three different metric patterns (two-, three-, and four-beat) superimposed on a steady metronome while electroencephalography (EEG) measurements were made. Shared information between perception and imagery EEG is investigated by means of principal component analysis and by means of single-trial classification.
Results
Classification of accented from non-accented beats was possible with an average accuracy of 70\% for perception and 61\% for imagery data. Cross-condition classification yielded significant performance above chance level for a classifier trained on perception and tested on imagery data (up to 66\%), and vice versa (up to 60\%).
Conclusions
Results show that detection of imagined accents is possible and reveal similarity in brain signatures relevant to distinction of accents from non-accents in perception and imagery.
Significance
Our results support the idea of shared mechanisms in perception and imagery for auditory processing. This is relevant for a number of clinical settings, most notably by elucidating the basic mechanisms of rhythmic auditory cuing paradigms, e.g. as used in motor rehabilitation or therapy for Parkinson's disease. As a novel Brain--Computer Interface (BCI) paradigm, our results imply a reduction of the necessary BCI training in healthy subjects and in patients.},
	Author = {Vlek, R. J. and Schaefer, R. S. and Gielen, C. C. A. M. and Farquhar, J. D. R. and Desain, P.},
	Date-Added = {2015-09-25 00:15:15 +0000},
	Date-Modified = {2015-09-25 00:15:15 +0000},
	Doi = {10.1016/j.clinph.2011.01.042},
	File = {ScienceDirect Snapshot:/Users/sstober/work/zotero/storage/K6585K2T/S1388245711000927.html:text/html},
	Issn = {1388-2457},
	Journal = {Clinical Neurophysiology},
	Keywords = {Auditory imagery, Cross-condition classification, Electroencephalography, Perception and imagery, Rhythm processing, Single-trial classification, Subjective rhythmization},
	Number = {8},
	Pages = {1526--1532},
	Title = {Shared mechanisms in perception and imagery of auditory accents},
	Url = {http://www.sciencedirect.com/science/article/pii/S1388245711000927},
	Urldate = {2015-04-26},
	Volume = {122},
	Xmonth = aug,
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1388245711000927},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.clinph.2011.01.042}}

@article{geiser_early_2009,
	Abstract = {The two main characteristics of temporal structuring in music are meter and rhythm. The present experiment investigated the event-related potentials (ERP) of these two structural elements with a focus on differential effects of attended and unattended processing. The stimulus material consisted of an auditory rhythm presented repetitively to subjects in which metrical and rhythmical changes as well as pitch changes were inserted. Subjects were to detect and categorize either temporal changes (attended condition) or pitch changes (unattended condition). Furthermore, we compared a group of long-term trained subjects (musicians) to non-musicians. As expected, behavioural data revealed that trained subjects performed significantly better than untrained subjects. This effect was mainly due to the better detection of the meter deviants. Rhythm as well as meter changes elicited an early negative deflection compared to standard tones in the attended processing condition, while in the unattended processing condition only the rhythm change elicited this negative deflection. Both effects were found across all experimental subjects with no difference between the two groups. Thus, our data suggest that meter and rhythm perception could differ with respect to the time course of processing and lend credence to the notion of different neurophysiological processes underlying the auditory perception of rhythm and meter in music. Furthermore, the data indicate that non-musicians are as proficient as musicians when it comes to rhythm perception, suggesting that correct rhythm perception is crucial not only for musicians but for every individual.},
	Author = {Geiser, Eveline and Ziegler, Esther and Jancke, Lutz and Meyer, Martin},
	Date-Added = {2015-09-25 00:15:07 +0000},
	Date-Modified = {2015-09-25 00:15:07 +0000},
	Doi = {10.1016/j.cortex.2007.09.010},
	File = {ScienceDirect Snapshot:/Users/sstober/work/zotero/storage/CX85KSAZ/S0010945208002475.html:text/html},
	Issn = {0010-9452},
	Journal = {Cortex},
	Keywords = {Brain, Perception, Plasticity, Rhythm, Timing},
	Number = {1},
	Pages = {93--102},
	Series = {Special {Issue} on "{The} {Rhythmic} {Brain}"},
	Title = {Early electrophysiological correlates of meter and rhythm processing in music perception},
	Url = {http://www.sciencedirect.com/science/article/pii/S0010945208002475},
	Urldate = {2015-04-26},
	Volume = {45},
	Xmonth = jan,
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0010945208002475},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.cortex.2007.09.010}}

@article{nozaradan_selective_2012,
	Abstract = {Fundamental to the experience of music, beat and meter perception refers to the perception of periodicities while listening to music occurring within the frequency range of musical tempo. Here, we explored the spontaneous building of beat and meter hypothesized to emerge from the selective entrainment of neuronal populations at beat and meter frequencies. The electroencephalogram (EEG) was recorded while human participants listened to rhythms consisting of short sounds alternating with silences to induce a spontaneous perception of beat and meter. We found that the rhythmic stimuli elicited multiple steady state-evoked potentials (SS-EPs) observed in the EEG spectrum at frequencies corresponding to the rhythmic pattern envelope. Most importantly, the amplitude of the SS-EPs obtained at beat and meter frequencies were selectively enhanced even though the acoustic energy was not necessarily predominant at these frequencies. Furthermore, accelerating the tempo of the rhythmic stimuli so as to move away from the range of frequencies at which beats are usually perceived impaired the selective enhancement of SS-EPs at these frequencies. The observation that beat- and meter-related SS-EPs are selectively enhanced at frequencies compatible with beat and meter perception indicates that these responses do not merely reflect the physical structure of the sound envelope but, instead, reflect the spontaneous emergence of an internal representation of beat, possibly through a mechanism of selective neuronal entrainment within a resonance frequency range. Taken together, these results suggest that musical rhythms constitute a unique context to gain insight on general mechanisms of entrainment, from the neuronal level to individual level.},
	Author = {Nozaradan, Sylvie and Peretz, Isabelle and Mouraux, Andr{\'e}},
	Date-Added = {2015-09-25 00:14:57 +0000},
	Date-Modified = {2015-09-25 00:14:57 +0000},
	Doi = {10.1523/JNEUROSCI.3203-12.2012},
	File = {Full Text PDF:/Users/sstober/work/zotero/storage/B4XQGQPI/Nozaradan et al. - 2012 - Selective Neuronal Entrainment to the Beat and Met.pdf:application/pdf;Snapshot:/Users/sstober/work/zotero/storage/F83DVQNC/17572.html:text/html},
	Issn = {0270-6474, 1529-2401},
	Journal = {The Journal of Neuroscience},
	Language = {en},
	Number = {49},
	Pages = {17572--17581},
	Pmid = {23223281},
	Title = {Selective {Neuronal} {Entrainment} to the {Beat} and {Meter} {Embedded} in a {Musical} {Rhythm}},
	Url = {http://www.jneurosci.org/content/32/49/17572},
	Urldate = {2015-04-26},
	Volume = {32},
	Xmonth = dec,
	Year = {2012},
	Bdsk-Url-1 = {http://www.jneurosci.org/content/32/49/17572},
	Bdsk-Url-2 = {http://dx.doi.org/10.1523/JNEUROSCI.3203-12.2012}}

@article{nozaradan_tagging_2011,
	Abstract = {Feeling the beat and meter is fundamental to the experience of music. However, how these periodicities are represented in the brain remains largely unknown. Here, we test whether this function emerges from the entrainment of neurons resonating to the beat and meter. We recorded the electroencephalogram while participants listened to a musical beat and imagined a binary or a ternary meter on this beat (i.e., a march or a waltz). We found that the beat elicits a sustained periodic EEG response tuned to the beat frequency. Most importantly, we found that meter imagery elicits an additional frequency tuned to the corresponding metric interpretation of this beat. These results provide compelling evidence that neural entrainment to beat and meter can be captured directly in the electroencephalogram. More generally, our results suggest that music constitutes a unique context to explore entrainment phenomena in dynamic cognitive processing at the level of neural networks.},
	Author = {Nozaradan, Sylvie and Peretz, Isabelle and Missal, Marcus and Mouraux, Andr{\'e}},
	Date-Added = {2015-09-25 00:14:57 +0000},
	Date-Modified = {2015-09-25 00:14:57 +0000},
	Doi = {10.1523/JNEUROSCI.0411-11.2011},
	Issn = {0270-6474},
	Journal = {The Journal of Neuroscience},
	Number = {28},
	Pages = {10234--10240},
	Pmid = {21753000},
	Title = {Tagging the neuronal entrainment to beat and meter.},
	Volume = {31},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1523/JNEUROSCI.0411-11.2011}}

@article{fujioka_internalized_2012,
	Abstract = {Moving in synchrony with an auditory rhythm requires predictive action based on neurodynamic representation of temporal information. Although it is known that a regular auditory rhythm can facilitate rhythmic movement, the neural mechanisms underlying this phenomenon remain poorly understood. In this experiment using human magnetoencephalography, 12 young healthy adults listened passively to an isochronous auditory rhythm without producing rhythmic movement. We hypothesized that the dynamics of neuromagnetic beta-band oscillations (∼20 Hz)-which are known to reflect changes in an active status of sensorimotor functions-would show modulations in both power and phase-coherence related to the rate of the auditory rhythm across both auditory and motor systems. Despite the absence of an intention to move, modulation of beta amplitude as well as changes in cortico-cortical coherence followed the tempo of sound stimulation in auditory cortices and motor-related areas including the sensorimotor cortex, inferior-frontal gyrus, supplementary motor area, and the cerebellum. The time course of beta decrease after stimulus onset was consistent regardless of the rate or regularity of the stimulus, but the time course of the following beta rebound depended on the stimulus rate only in the regular stimulus conditions such that the beta amplitude reached its maximum just before the occurrence of the next sound. Our results suggest that the time course of beta modulation provides a mechanism for maintaining predictive timing, that beta oscillations reflect functional coordination between auditory and motor systems, and that coherence in beta oscillations dynamically configure the sensorimotor networks for auditory-motor coupling.},
	Author = {Fujioka, T. and Trainor, L. J. and Large, E. W. and Ross, B.},
	Date-Added = {2015-09-25 00:14:47 +0000},
	Date-Modified = {2015-09-25 00:14:47 +0000},
	Doi = {10.1523/JNEUROSCI.4107-11.2012},
	Issn = {0270-6474},
	Journal = {Journal of Neuroscience},
	Number = {5},
	Pages = {1791--1802},
	Pmid = {22302818},
	Title = {Internalized {Timing} of {Isochronous} {Sounds} {Is} {Represented} in {Neuromagnetic} {Beta} {Oscillations}},
	Volume = {32},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1523/JNEUROSCI.4107-11.2012}}

@article{fujioka_beta_2009,
	Abstract = {We examined beta- (approximately 20 Hz) and gamma- (approximately 40 Hz) band activity in auditory cortices by means of magnetoencephalography (MEG) during passive listening to a regular musical beat with occasional omission of single tones. The beta activity decreased after each tone, followed by an increase, thus forming a periodic modulation synchronized with the stimulus. The beta decrease was absent after omissions. In contrast, gamma-band activity showed a peak after tone and omission, suggesting underlying endogenous anticipatory processes. We propose that auditory beta and gamma oscillations have different roles in musical beat encoding and auditory-motor interaction.},
	Author = {Fujioka, Takako and Trainor, Laurel J. and Large, Edward W. and Ross, Bernhard},
	Date-Added = {2015-09-25 00:14:42 +0000},
	Date-Modified = {2015-09-25 00:14:42 +0000},
	Doi = {10.1111/j.1749-6632.2009.04779.x},
	Issn = {00778923},
	Journal = {Annals of the New York Academy of Sciences},
	Keywords = {Auditory-motor interaction, Event-related desynchronization (ERD), Magnetoencephalography, Musical beat, Neural oscillation},
	Pages = {89--92},
	Pmid = {19673759},
	Title = {Beta and gamma rhythms in human auditory cortex during musical beat processing},
	Volume = {1169},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1111/j.1749-6632.2009.04779.x}}

@article{iversen_top-down_2009,
	Abstract = {Our perceptions are shaped by both extrinsic stimuli and intrinsic interpretation. The perceptual experience of a simple rhythm, for example, depends upon its metrical interpretation (where one hears the beat). Such interpretation can be altered at will, providing a model to study the interaction of endogenous and exogenous influences in the cognitive organization of perception. Using magnetoencephalography (MEG), we measured brain responses evoked by a repeating, rhythmically ambiguous phrase (two tones followed by a rest). In separate trials listeners were instructed to impose different metrical organizations on the rhythm by mentally placing the downbeat on either the first or the second tone. Since the stimulus was invariant, differences in brain activity between the two conditions should relate to endogenous metrical interpretation. Metrical interpretation influenced early evoked neural responses to tones, specifically in the upper beta range (20-30 Hz). Beta response was stronger (by 64\% on average) when a tone was imagined to be the beat, compared to when it was not. A second experiment established that the beta increase closely resembles that due to physical accents, and thus may represent the genesis of a subjective accent. The results demonstrate endogenous modulation of early auditory responses, and suggest a unique role for the beta band in linking of endogenous and exogenous processing. Given the suggested role of beta in motor processing and long-range intracortical coordination, it is hypothesized that the motor system influences metrical interpretation of sound, even in the absence of overt movement.},
	Author = {Iversen, John R. and Repp, Bruno H. and Patel, Aniruddh D.},
	Date-Added = {2015-09-25 00:14:34 +0000},
	Date-Modified = {2015-09-25 00:14:34 +0000},
	Doi = {10.1111/j.1749-6632.2009.04579.x},
	Issn = {00778923},
	Journal = {Annals of the New York Academy of Sciences},
	Keywords = {Auditory, Beat, Beta, Cortex, ERF, ERP, Evoked, Gamma, Magnetoencephalography, MEG, Meter, Metrical interpretation, Music, Rhythm, Subjective accent},
	Pages = {58--73},
	Pmid = {19673755},
	Title = {Top-down control of rhythm perception modulates early auditory responses},
	Volume = {1169},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1111/j.1749-6632.2009.04579.x}}

@article{merchant_beta_2015,
	Author = {Merchant, Hugo and Grahn, Jessica and Trainor, Laurel J. and Rohrmeier, Martin and Fitch, W. Tecumseh},
	Date-Added = {2015-09-25 00:14:22 +0000},
	Date-Modified = {2015-09-25 00:14:22 +0000},
	Journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	Title = {Finding a beat: a neural perspective across humans and non-human primates},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QHy4uLy4uLy4uL0Rvd25sb2Fkcy9VbnRpdGxlZC5iaWLSFwsYGVdOUy5kYXRhTxEBhAAAAAABhAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAy+IPgEgrAAAABfnEDFVudGl0bGVkLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY9QOnSKgqOAAAAAAAAAAAAAwACAAAJIAAAAAAAAAAAAAAAAAAAAAlEb3dubG9hZHMAABAACAAAy+JHwAAAABEACAAA0ipCzgAAAAEADAAF+cQABfliAAC//gACADpNYWNpbnRvc2ggSEQ6VXNlcnM6AGF2aXRhbHN0ZXJuaW46AERvd25sb2FkczoAVW50aXRsZWQuYmliAA4AGgAMAFUAbgB0AGkAdABsAGUAZAAuAGIAaQBiAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAKlVzZXJzL2F2aXRhbHN0ZXJuaW4vRG93bmxvYWRzL1VudGl0bGVkLmJpYgATAAEvAAAVAAIAFP//AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOALAAtQC9AkUCRwJMAlcCYAJuAnICeQKCAocClAKXAqkCrAKxAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAArM=}}

@article{cirelli_beta_2014,
	Author = {Cirelli, Laura K. and Bosnyak, Dan and Manning, Fiona C. and Spinelli, Christina and Marie, C\'{e}line and Fujioka, Takako and Ghahremani, Ayda and Trainor, Laurel J.},
	Date-Added = {2015-09-25 00:14:17 +0000},
	Date-Modified = {2015-09-25 00:14:17 +0000},
	Doi = {10.3389/fpsyg.2014.00742},
	File = {:Users/avitalsternin/Documents/Mendeley Articles/Cirelli et al. - 2014 - Beat-induced fluctuations in auditory cortical beta-band activity using EEG to measure age-related changes.pdf:pdf},
	Isbn = {1664-1078 (Electronic)},
	Issn = {16641078},
	Journal = {Frontiers in Psychology},
	Keywords = {Child development,Electroencephalography (EEG),Musical rhythm,Neural oscillation,Time perception},
	Number = {Jul},
	Pages = {1--9},
	Pmid = {25071691},
	Title = {{Beat-induced fluctuations in auditory cortical beta-band activity: Using EEG to measure age-related changes}},
	Volume = {5},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.3389/fpsyg.2014.00742}}

@article{snyder_gamma-band_2005,
	Abstract = {Relatively little is known about the dynamics of auditory cortical rhythm processing using non-invasive methods, partly because resolving responses to events in patterns is difficult using long-latency auditory neuroelectric responses. We studied the relationship between short-latency gamma-band (20-60 Hz) activity (GBA) and the structure of rhythmic tone sequences. We show that induced (non-phase-locked) GBA predicts tone onsets and persists when expected tones are omitted. Evoked (phase-locked) GBA occurs in response to tone onsets with ∼50 ms latency, and is strongly diminished during tone omissions. These properties of auditory GBA correspond with perception of meter in acoustic sequences and provide evidence for the dynamic allocation of attention to temporally structured auditory sequences. {\textbackslash}copyright 2005 Elsevier B.V. All rights reserved.},
	Author = {Snyder, Joel S. and Large, Edward W.},
	Date-Added = {2015-09-25 00:14:10 +0000},
	Date-Modified = {2015-09-25 00:14:10 +0000},
	Doi = {10.1016/j.cogbrainres.2004.12.014},
	Issn = {09266410},
	Journal = {Cognitive Brain Research},
	Keywords = {Electroencephalography, Gamma-band activity, Music, Rhythm perception, Speech},
	Pages = {117--126},
	Pmid = {15922164},
	Title = {Gamma-band activity reflects the metric structure of rhythmic tone sequences},
	Volume = {24},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.cogbrainres.2004.12.014}}

@inproceedings{stober2014nips,
	Author = {Sebastian Stober and Daniel J. Cameron and Jessica A. Grahn},
	Booktitle = {Advances in Neural Information Processing Systems 27 (NIPS'14)},
	Date-Added = {2015-09-25 00:14:04 +0000},
	Date-Modified = {2015-09-25 00:14:04 +0000},
	Pages = {1449--1457},
	Title = {Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings},
	Year = {2014}}

@inproceedings{cabredo_emotion_2012,
	Author = {Cabredo, Rafael and Legaspi, Roberto S. and Inventado, Paul Salvador and Numao, Masayuki},
	Booktitle = {Proceedings of the 13th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR}'12)},
	Date-Added = {2015-09-25 00:13:54 +0000},
	Date-Modified = {2015-09-25 00:13:54 +0000},
	Pages = {265--270},
	Title = {An {Emotion} {Model} for {Music} {Using} {Brain} {Waves}},
	Url = {http://ismir2012.ismir.net/event/papers/265-ismir-2012.pdf},
	Urldate = {2015-04-26},
	Xbooktitle = {Proceedings of the 13th {International} {Society} for {Music} {Information} {Retrieval} {Conference}, {ISMIR} 2012, {Mosteiro} {S}.{Bento} {Da} {Vit{\'o}ria}, {Porto}, {Portugal}, {October} 8-12, 2012},
	Xeditor = {Gouyon, Fabien and Herrera, Perfecto and Martins, Luis Gustavo and M{\"u}ller, Meinard},
	Xisbn = {978-972-752-144-9},
	Xpublisher = {FEUP Edi{\c c}{\~o}es},
	Year = {2012},
	Bdsk-Url-1 = {http://ismir2012.ismir.net/event/papers/265-ismir-2012.pdf}}

@inproceedings{lin_eeg_2009,
	Abstract = {This study explores the electroencephalographic (EEG) correlates of emotions during music listening. Principal component analysis (PCA) is used to correlate EEG features with complex music appreciation. This study also applies machine-learning algorithms to demonstrate the feasibility of classifying EEG dynamics in four subjectively-reported emotional states. The high classification accuracy (81.58plusmn3.74\%) demonstrates the feasibility of using EEG features to assess emotional states of human subjects. Further, the spatial and spectral patterns of the EEG most relevant to emotions seem reproducible across subjects.},
	Author = {Lin, Yuan-Pin and Jung, Tzyy-Ping and Chen, Jyh-Horng},
	Booktitle = {Annual {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC}'09)},
	Date-Added = {2015-09-25 00:13:49 +0000},
	Date-Modified = {2015-09-25 00:13:49 +0000},
	Doi = {10.1109/IEMBS.2009.5333524},
	File = {IEEE Xplore Abstract Record:/Users/sstober/work/zotero/storage/66K6FZFB/freeabs_all.html:text/html},
	Keywords = {Brain Mapping, Cognition, complex music appreciation, correlation methods, EEG dynamics, EEG feature classification, Electrodes, electroencephalographic correlation, Electroencephalography, Emotions, feature extraction, Humans, learning (artificial intelligence), machine-learning algorithm, medical signal processing, Music, music listening, neurophysiology, PCA, Principal component analysis, signal classification, spatial patterns, spectral analysis, spectral patterns, subjectively-reported emotional states},
	Pages = {5316--5319},
	Title = {{EEG} dynamics during music appreciation},
	Xmonth = sep,
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IEMBS.2009.5333524}}
